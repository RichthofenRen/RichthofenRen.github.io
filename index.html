
<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yihui He, CS undergrad interested in Computer vision & Deep Learning</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    <link rel="icon" href="./assets_files/favicon.ico">
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#research">Research</a>-->
    <a href="#publications">Publications</a>
    <a href="#projects">Projects</a>
    <!--<a href="#teaching">Teaching</a>-->
    <a target="_blank"

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Yangqing Jia</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="assets/css/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="container">
      <div class="row">
        <div class="span3 bs-docs-sidebar">
          <!-- We use a fancy nav bar if there is enough space -->
          <hr class="hidden-phone"/>
          <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="index.html"><i class="icon-play"></i>Home</a></li>
            <li><a href="index.html#research"><i class="icon-play"></i>Research</a></li>
            <li><a href="index.html#publications"><i class="icon-play"></i>Publications</a></li>
            <li><a href="index.html#software"><i class="icon-play"></i>Software</a></li>
            <li><a href="index.html#teaching"><i class="icon-play"></i>Teaching</a></li>
            <li><a href="assets/pdf/CV.pdf"><i class="icon-play"></i>CV</a></li>
          </ul>
          <hr class="hidden-phone"/>
          <div class="text-center hidden-phone">
            <img src="assets/img/Yangqing.png" alt="photo" class="logo-image"/>
          </div>
          <!-- Otherwise, we simply use a flat list of links -->
          <div class="visible-phone">
            <a href="index.html"><i class="icon-play"></i>Home</a>
            <a href="index.html#research"><i class="icon-play"></i>Research</a>
            <a href="index.html#publications"><i class="icon-play"></i>Publications</a>
            <a href="index.html#software"><i class="icon-play"></i>Software</a>
            <a href="index.html#teaching"><i class="icon-play"></i>Teaching</a>
            <a href="assets/pdf/CV.pdf"><i class="icon-play"></i>CV</a>
          </div>
        </div>
        <div class="span9">
          <h3>
	Yangqing Jia
	(贾扬清<a href="http://en.wikipedia.org/wiki/Chinese_character"><i class="icon-question-sign"></i></a>)
</h3>
<h5>
<a href="mailto:me@daggerfs.com">me@daggerfs.com</a>
</h5>
<!-- Do I want to show a pic on the phone screen?
<div class="text-center visible-phone">
	<img src="assets/img/Yangqing.png" alt="photo" width="150px"/>
</div>
-->
<a class="visible-phone pull-left" href="#">
  <img class="media-object" src="assets/img/Yangqing.png" width="96px" style="margin: 0px 10px" />
</a>
<p>
I am currently a research scientist at Facebook, where I lead the effort of building a general, large-scale platform for the many AI applications at Facebook. Previously, I was a research scientist at Google Brain where I worked on computer vision, deep learning and TensorFlow.
</p>
<p>
I obtained my Ph.D. in Computer Science at UC Berkeley, advised by Prof. <a href="http://eecs.berkeley.edu/~trevor/">Trevor Darrell</a>. During my graduate study I've worked/interned at the National University of Singapore, Microsoft Research Asia, NEC Labs America, and Google Research. I obtained my bachelor and master degrees from Tsinghua University, China.</p>
<p>
I am the author of <a href="http://caffe.berkeleyvision.org/">Caffe</a>, which is now a <a href="http://bvlc.eecs.berkeley.edu/">BVLC</a> maintained, open-source deep learning framework. I've worked on the <a href="https://www.tensorflow.org/">TensorFlow</a> project at Google Brain.
</p>

<p>
	I do have a <a href="http://www.linkedin.com/pub/yangqing-jia/b/37/a67">LinkedIn profile</a>.

</p>



<!--
 *** Research ***
-->
<h3>
	<a name='research'></a> Research 
</h3>
<p>
My current research topics include:
    <ul>
        <li> Learning better structures for image feature extraction.
        <li> Explaining human generalization behavior with visually grounded cogscience models.
        <li> Making large-scale vision feasible and affordable.
    </ul>
</p>




<!--
 *** Publications ***
-->
<p> (Most recent publications to be added) </p>
<h3>
	<a name='publications'></a> Recent Publications 
</h3>
<p>
    Links to: <a href="publications.html">[Full List]</a>
	<a href="http://scholar.google.com/citations?user=mu5Y2rYAAAAJ&hl=en" target="_blank">[Google Scholar]</a>
    <!--<a href="projects.html"> [Unpublished Projects]</a>-->
</p>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/decaf-features.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br />
      J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br />
      <a href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>
      <a href="http://decaf.berkeleyvision.org/">[Live Demo]</a>
      <a href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>
      <a href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>
    </p>
    <p class="abstract-text">
    We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/mrFrog.jpg" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</strong><br />
      Y Jia, J Abbott, J Austerweil, T Griffiths, T Darrell. NIPS 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    It is marvelous that human can learn concept from a small number of examples, a challenge many existing machine vision systems fail to do. We present a system combining computer vision and cogscience to model such human behavior, as well as a new dataset for future experientation on human concept learning.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv13_ta.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Latent Task Adaptation with Large-scale Hierarchies</strong><br />
      Y Jia, T Darrell. ICCV 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    How do we adapt our ImageNet classifiers to accurately classify just giraffes and bears on a zoo trip? We proposed a novel framework that benefits from big training data and adaptively adjusts itself for subcategory test scenarios. 
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv13_sa.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Category Independent Object-level Saliency Detection</strong><br />
      Y Jia, M Han. ICCV 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    We proposed a simple yet efficient approach to combine high-level object models and low-level appearance information to perform saliency detection that identifies foreground objects.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/icml13.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>On Compact Codes for Spatially Pooled Features</strong><br />
      Y Jia, O Vinyals, T Darrell. ICML 2013.
      <a href="http://jmlr.org/proceedings/papers/v28/jia13.pdf">[PDF]</a>
      <a href="assets/pdf/icml13_poster.pdf">[poster]</a>
      <a href=http://arxiv.org/abs/1301.5348>[ICLR workshop version]</a>
    </p>
    <p class="abstract-text">
      We analyzed the connection between codebook size and accuracy with the Nystrom sampling theory, and showed how this leads to better pooling-aware codebook learning methods.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/nips12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Learning with Recursive Perceptual Representations</strong><br />
      O Vinyals, Y Jia, L Deng, T Darrell. NIPS 2012.
      <a href="assets/pdf/nips12_rsvm.pdf">[PDF]</a>
      <a href="assets/pdf/nips12_rsvm_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We proposed R2SVM, an efficient algorithm to recursively learn deep nonlinear models by stacking linear SVMs with random projections.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/cvpr12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features</strong><br />
      Y Jia, C Huang, T Darrell. CVPR 2012.
      <a href="assets/pdf/cvpr12_pooling.pdf">[PDF]</a>
      <a href="assets/pdf/cvpr12_pooling_slides.pdf">[Slides]</a>
      <a href="assets/pdf/cvpr12_pooling_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We showed the suboptimality of spatial pyramids in feature pooling, and proposed an efficient way to learn task-dependent receptive fields for better pooled features.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/uai12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Factorized Multi-modal Topic Model</strong><br />
      S Virtanen, Y Jia, A Klami, T Darrell. UAI 2012.
      <a href="assets/pdf/uai12_factorize.pdf">[PDF]</a>
      <!--<a href="wikipedia.html">[Dataset]</a> -->
    </p>
    <p class="abstract-text">
      We factorized the information contained in corresponding image and text with a novel HDP-based topic model that automatically learns both shared and private topics.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/nips11.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Heavy-tailed Distances for Gradient Based Image Descriptors</strong><br />
      Y Jia, T Darrell. NIPS 2011.
      <a href="assets/pdf/nips11_gcl.pdf">[PDF]</a>
      <a href="assets/pdf/nips11_gcl_supp.pdf">[Supplementary Material]</a>
      <a href="assets/pdf/nips11_gcl_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We examined the heavy-tailed noise distribution of gradient-based image descriptors, and proposed a new distance metric that yields higher feature matching performances.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv11.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Learning Cross-modality Similarity for Multinomial Data</strong><br />
      Y Jia, M Salzmann, T Darrell. ICCV 2011
      <a href="assets/pdf/iccv11_mm.pdf">[PDF]</a>
      <a href="assets/pdf/iccv11_mm_poster.pdf">[Poster]</a>
      <!--<a href="wikipedia.html">[Dataset]</a> -->
    </p>
    <p class="abstract-text">
      We propose a novel approach based on topic models and the Markov random field to capture the semantic relationships between documents from multiple modalities. 
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv11-b3do.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>A Category-level 3-D Database: Putting the Kinect to Work</strong><br />
      A Janoch, S Karayev, Y Jia, J Barron, M Fritz, K Saenko, T Darrell. ICCV-CDC4CV workshop 2011
      <a href="assets/pdf/iccv11_kinect.pdf">[PDF]</a>
      <a href="http://kinectdata.com/">[Dataset]</a>
    </p>
    <p class="abstract-text">
      We presented a dataset of color and depth image pairs collected from the Kinect sensor, gathered in real domestic and ofﬁce environments, for research on object-level recognition with multimodal sensor input.
    </p>
  </div>
</div>




<!--
 *** Software ***
-->
<h3>
	<a name='software'></a> Software 
</h3>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/mincepie.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>
      	<a href="https://github.com/Yangqing/mincepie">Mincepie: lightweighted Mapreduce</a>
      </strong>
    </p>
    <p class="abstract-text">
      A lightweighted mapreduce implementation purely written in Python. It's not the powerful yellow elephant, but is worth 30 minutes' learning time. Check <a href="https://gist.github.com/Yangqing/5596077" target="_blank">the script</a> that extracts gist features from all ImageNet images!
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/decaf.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>
      	<a href="http://decaf.no-ip.org">Decaf</a>
      </strong>
    </p>
    <p class="abstract-text">
    Decaf is a general python framework for deep convolutional neural networks, relying on a set of scientific computation modules (such as numpy/scipy) to efficiently run CNN models without the need of a GPU. Decaf is still under development but an imagenet classification demo could be checked out <a href="http://decaf.berkeleyvision.org">here</a>.
    </p>
  </div>
</div>


<!--
 *** Teaching ***
-->
<h3>
	<a name='teaching'></a> Teaching (GSI)
</h3>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/cs188.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>
      	<a href="http://inst.eecs.berkeley.edu/~cs188/sp12/">CS188 Artificial Intelligence</a>
      </strong>, spring 2012.
    </p>
    <p class="abstract-text">
      Undergraduate AI course: search, CSP, games, MDP, Reinforcement Learning, Bayes' Nets, HMM, DBN, probabilistic inference, and a fun PacMan challenge.<br />
      Won the campus Outstanding GSI Award.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/cs281a.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>
      	<a href="http://inst.eecs.berkeley.edu/~cs281a/"> CS281a/Stat241a Statistical Learning Theory</a>
      </strong>, fall 2011.
    </p>
    <p class="abstract-text">
      Graduate level course: graphical models, probabilistic inference, parameter estimation, regression, exponential family, EM and HMM, factor analysis, Junction Tree Algorithm, Monte Carlo, Variational Inference, etc.
    </p>
  </div>
</div>

        </div>
      </div>
    </div>

    <!-- Footer
    ================================================== -->
    <hr>
    <footer class="footer">
    <div class="container">
      <div class="row">
        <div class="span12">
          <p>&copy; Yangqing Jia 2013</p>
        </div>
      </div>
    </div>
    </footer>



    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
  </body>
</html>


        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Yihui He (何宜晖)
            </h3>
            <h5>
                yihuihe at foxmail 。com</a>
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="http://daggerfs.com/index.html#">
                <img class="media-object" src="assets_files/me.png" width="96px" style="margin: 0px 10px">
            </a>
            <p>
                I'm a Computer Science 3rd-year undergrad from <a
                    href="https://en.wikipedia.org/wiki/Xi%27an_Jiaotong_University">Xi'an Jiaotong University</a>, with
                my interest focus on Computer Vision, Deep Learning and Network Security.
            </p>
            <p>
                I'm currently a Computer Vision research intern at <a target="_blank"
                                                                      href="http://www.faceplusplus.com/">Megvii(Face++)</a>,
                supervised by Xiangyu Zhang, and <a target="_blank" href="http://jiansun.org/">Jian Sun</a>.
                I've interned at <a target="_blank" href="http://www.deepglint.com/en/">DeepGlint</a> as a Computer
                Vision intern, supervised by Debing Zhang. I studied at <a target="_blank"
                                                                           href="https://en.wikipedia.org/wiki/University_of_California,_Santa_Barbara">UCSB</a>
                grad school as an exchange student.</p>
            <p>Here's my <a target="_blank"
                            href="http://nbviewer.jupyter.org/github/yihui-he/resume-template/blob/master/examples/cv.pdf">CV</a>.
                I have strong interest in research, planning to pursue a Ph.D.
            </p>


            <!--
             *** Research ***
            -->
            <!--<h3>-->
            <!--<a name="research"></a> Research-->
            <!--</h3>-->
            <!--<p>-->
            <!--My current research topics include:-->
            <!--</p><ul>-->
            <!--<li> Learning better structures for image feature extraction.-->
            <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
            <!--</li><li> Making large-scale vision feasible and affordable.-->
            <!--</li></ul>-->
            <!--<p></p>-->
            <!--<p> (Most recent publications to be added) </p>-->


            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Recent Publications
            </h3>
            <p>
                Link to <a target="_blank"
                                           href="https://scholar.google.com/citations?user=z2w3scIAAAAJ&amp;hl=en"
                                           target="_blank">[Google Scholar]</a>
                <!--<a href="projects.html"> [Unpublished Projects]</a>-->
            </p>
            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/vehicle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>Vehicle Traffic Driven Camera Placement for Better Metropolis Security
                            Surveillance</strong><br>
                        Xiaobo Ma, Yihui He, Xiapu Luo, Jianfeng Li, Xiaohong Guan
                        [Contact me if interested] under review, IEEE Network
                    </p>
                    <p class="abstract-text">
                        Security surveillance is one of the most important issues in smart cities, especially in an era
                        of
                        terrorism. Deploying a number of (video) cameras is a
                        common approach for surveillance information retrieval.
                        Given the never-ending power offered by vehicles to a
                        metropolis, exploiting vehicle traffic to design camera
                        placement strategies could potentially facilitate physicalworld security surveillance. We take
                        the first step towards
                        exploring the linkage between vehicle traffic and camera
                        placement in favor of physical-world security surveillance
                        from a network perspective.
                    </p>
                </div>
            </div>

            <!--<div class="media">-->
            <!--<a class="pull-left" href="#top">-->
            <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
            <!--</a>-->
            <!--<div class="media-body">-->
            <!--<p class="media-heading">-->
            <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
            <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
            <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
            <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
            <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
            <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
            <!--</p>-->
            <!--<p class="abstract-text">-->
            <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
            <!--</p>-->
            <!--</div>-->
            <!--</div>-->

            <!--
             *** Projects ***
            -->
            <h3>
                <a name="projects"></a> Projects
            </h3>
            Link to my <a target="_blank" href="https://github.com/yihui-he">[github public projects]</a>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/TSP-report/master/02132.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            <a target="_blank" href="https://github.com/yihui-he/TSP">A Survey on Solutions for Traveling Salesman Problem</a>
                        </strong>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/TSP-paper/blob/master/egpaper_final.pdf">[Draft]</a>
                    </p>
                    <p class="abstract-text">
we perform an evaluation and analysis of cornerstone algorithms for the metric TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use
several datasets as input for the algorithms including a small dataset, a medium-sized dataset representing
cities in the United States, and a synthetic dataset consisting of 200 cities to test algorithm scalability.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            <a target="_blank" href="https://github.com/yihui-he/Depth-estimation-with-neural-network">Images Classification with Estimated Depth Map</a>
                        </strong>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/Depth-estimation-with-neural-network/blob/master/latex/egpaper_final.pdf">[Draft]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/Depth-estimation-with-neural-network/blob/master/presentation/depth.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        Previous related research efforts have been focused on image classification tasks using
RGB images and depth image estimation but none have attempted to use depth image estimations in order to aid image classification over RGB images. Therefore, in this paper we present a way of 
transferring domain knowledge on depth estimation to a seperate image classification task over a disjoint set of training,validation and test data. 
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://www.kaggle.io/svf/310043/4567203286d71c8fd31cf12668a3ceac/__results___files/__results___5_0.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            <a target="_blank" href="https://github.com/yihui-he/Ultrasound-Nerve-Segmentation">Medical
                                Image Segmentation: A survey</a>
                        </strong>
                        <a target="_blank"
                           href="https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md">[Summary]</a>
                    </p>
                    <p class="abstract-text">
                        I evaluate DeepMask, Deeplab and MNC for medical image segmentation. I ranked
                        <strong>19%</strong> on <a target="_blank"
                                                   href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/leaderboard/private">Kaggle
                        Ultrasound Nerve Segmentation</a>
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            <a target="_blank" href="https://github.com/yihui-he/Residual-neural-network">residual
                                neural network with tensorflow on CIFAR100 </a>
                        </strong>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/Residual-neural-network/blob/master/report/mp2_Yihui%20He.pdf">[draft]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        I reimplement 6/13/20 layers RNN in tensorflow from scratch, and test it’s result on
                        CIFAR10/100.
                    </p>
                </div>
            </div>


            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"
                         width="96px" height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong><a target="_blank"
                                   href="https://github.com/yihui-he/Single-Layer-neural-network-with-PCAwhitening-Kmeans">Single
                            Layer neural network with PCAwhitening Kmeans</a></strong> <a target="_blank"
                                                                                          href="http://nbviewer.jupyter.org/github/yihui-he/Single-Layer-neural-network-with-PCAwhitening-Kmeans/blob/master/report/mp1_Yihui%20He.pdf">[draft]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        We evaluate whether features extracted from the activation of a deep convolutional network
                        trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be
                        re-purposed to novel generic tasks. We also released the software and pre-trained network to do
                        large-scale image classification.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png" width="96px"
                         height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong><a target="_blank"
                                   href="https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset">Objects
                            Detection with YOLO on Artwork Dataset</a></strong> <a target="_blank"
                                                                                   href="http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf">[draft]</a>
                    </p>
                    <p class="abstract-text">
                        I design a small object detection network, which is simplified from YOLO(You Only Look Once)
                        network. It's trained on PASCAL VOC. I evaluate it on an artwork dataset(Picasso dataset). With
                        the best parameters, I got 40% precision and 35% recall.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/shuttle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>shuttlecock detection and tracking</strong>
                    </p>
                    <p class="abstract-text">
                        With guassian mixture model, I extract shuttlecock proposals. Then I use Partical filter to
                        refine proposals. From multi view cameras, I employed
                        structure from motion to predict its 3D location. Combined with Physics laws, landing location
                        prediction accuracy is around 5 cm. (This system
                        works on embeded linux with openCV)
                    </p>
                </div>
            </div>
            <!-- Footer
            ================================================== -->
            <hr>
            <footer class="footer">
                <div class='hidden-phone'>
                <h3 class="text-center"><a name="wall"></a><strong>works</strong></h3>
                <section id="photos">
                    <img src="https://raw.githubusercontent.com/yihui-he/lip-tracking-with-snake-active-contour-and-particle-filter/master/pic.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Edge-detection-with-zero-crossing/master/lena_1.bmp"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/3D-reconstruction/master/result/selfff.png"/>
                    <img src="./assets_files/cs188.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png"/>
                    <img src="http://students.iitk.ac.in/robocon/images/sliders/master/bg-5.jpg"/>
                    <img src="./assets_files/vehicle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"/>
                    <img src="./assets_files/shuttle.png"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/gaoxin.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/artwork.jpg"/>
                    <img src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/bop.jpg"/>
                    <img src="./assets_files/ocsi.png"/>
                </section>
                
                <a target="_blank" href="https://github.com/yihui-he/panorama"><img
                        src="https://github.com/yihui-he/panorama/blob/master/results/yellowstone5.jpg?raw=true"></a>
                <hr>
                </div>
                <div class="row">
                    <div class="span12">
                        <p>
                            modified from <a target="_blank" href="http://daggerfs.com/">© Yangqing Jia 2013</a>
                        </p>
                    </div>
                </div>

            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->

